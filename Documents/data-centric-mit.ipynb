{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data-Centric AI, MIT IAP 2023."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 1: Data-Centric AI vs. Model-Centric AI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is not data centric AI:\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Hand-picking a bunch of data points you think will improve the model.\n",
    "2. Double the size of your dataset and train an improved model on more data.\n",
    "\n",
    "Data centric version:\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Coreset selection: coreset selection in supervised learning is to produce a weighted subset of data, so that training only on the subset achieves similar performance as training on the entire dataset ***Look for testing it***.\n",
    "\n",
    "2. Dataset augmentation.\n",
    "3. outlier detection and removal.\n",
    "4. Feature selection.\n",
    "5. Establishing consensus labels.\n",
    "6. active learning (selecting the most informative data to label next).\n",
    "7. curriculum learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 2: Label Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding label erros by sorting data by loss**\n",
    "\n",
    "- how much error is in the dataset.\n",
    "- then, define which labels have more error. sort!\n",
    "- whats the cutt-off?.\n",
    "\n",
    "**What is confident learning**\n",
    "\n",
    "Confident learning is a framework of theory and algorithms for:\n",
    "\n",
    "- Finding label erros in a dataset.\n",
    "- Ranking data by likehood of being a label issue.\n",
    "- Learning with noise labels.\n",
    "- ***Complete characterization of label noise in a dataset*** (focus here!!).\n",
    "\n",
    "key idea:\n",
    "\n",
    "    With confident learning, any models predicted probabilities to find label errors. (data centric, model agnostic)\n",
    "\n",
    "**Where do noise labels come from??**\n",
    "\n",
    "- clicked the wrong bottom.\n",
    "- mistakes.\n",
    "- mismeasurement.\n",
    "- incompetence.\n",
    "- another ml model's bad predictions.\n",
    "\n",
    "All of these results in label flippigns.\n",
    "\n",
    "Example of label flippings:\n",
    "\n",
    "- Image of a Dog is labeled in fox.\n",
    "- Tweet \"Hi Welcome to the team\" is labeled toxic language.\n",
    "\n",
    "**How noise labels are generated**\n",
    "\n",
    "- uniform/symmetric class-conditional label noise.\n",
    "- systematic/assymetric class-conditional label noise.\n",
    "- instance-dependent label noise.\n",
    "\n",
    "**What's uncertainty**\n",
    "\n",
    "Its the opposite of confidence (lack of confidence). Depends on:\n",
    "\n",
    "- the difficult of an example (aletoric - label noise: labels have been flipped to another class).\n",
    "- missunderstaing the example (epistemic - model noise: erroneus predicted probabilities).\n",
    "\n",
    "**CL assumes class-conditional label noise**\n",
    "\n",
    "We assume labels are flipped based on an unknown transition matrix P(y~|y*) that depends only on pairwase noise rates between classes, not the data `x`. The `class conditional` label noise depends on the class, not the data.\n",
    "\n",
    "`Deep learning is robust to label noise`. These results assume uniformly random label noise and usually don't apply to real world seetings.\n",
    "\n",
    "**How does confident learning work**\n",
    "\n",
    "Directly estimate the joint distribution of observed noise labels and latent true labels.\n",
    "\n",
    "__key_idea__: First we find the threshold as a proxy for the machine's self confidence, on average, for each task/class j (you can apply cross-validation to get out-of-sample predicted probabilities. Then average the preidcted probabilities by class).\n",
    "\n",
    "$$\n",
    "  t_{j} = \\frac{1}{|X_{\\hat{y}=j}|} \\sum_{x \\in X_{\\hat{y}=j}} \\mathbb P(\\hat{y}=j; x; O)\n",
    "$$\n",
    "\n",
    "```python\n",
    "\n",
    "from  cleanlab.filters import find_label_issues\n",
    "\n",
    "#! Works with any ml model -  just input the model's predicted probabilities\n",
    "\n",
    "ordered_label_issues = find_label_issues(\n",
    "  labels=labels,\n",
    "  pred_probs=pred_probs, # out-of-sample predicted probabilities from any model\n",
    "  return_indices_ranked_by='self_confidence'\n",
    ")\n",
    "```\n",
    "\n",
    "**Ranking label errors**\n",
    "\n",
    "Self confidence.\n",
    "\n",
    "$$\n",
    "  \\mathbb P(\\hat{y}=i; x)\n",
    "$$\n",
    "\n",
    "Normalized margin (with formula). Then, sort it!!\n",
    "\n",
    "$$\n",
    "  \\mathbb P(\\hat{y} = i) - \\max_{i \\in m}(\\mathbb P(\\hat{y}))\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 3: Dataset Creation and Curation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concerns when sourcing the data**\n",
    "\n",
    "Key questions when sourcing training data\n",
    "\n",
    "1. How will the resulting ml model be used?\n",
    "    - On what population will model be making predictions and when.\n",
    "\n",
    "2. Hypothetical edge cases where we need model make right predictions.\n",
    "    - High stakes scenarios, rare events.\n",
    "\n",
    "sparse correlation: Ml Model = shortcut cheaters.\n",
    "\n",
    "selection bias: training delta distribution != distribbution in deployment.\n",
    "\n",
    "causes:\n",
    "\n",
    "    - time.\n",
    "    - overfitting.\n",
    "    - rare events.\n",
    "    - Convenience.\n",
    "    - Location.\n",
    "\n",
    "`Validation data` should be similar to distribution in deployment (use most recent data).\n",
    "\n",
    "__How much data should be collected??__\n",
    "\n",
    "Goal: 95% accuracy.\n",
    "\n",
    "One idea: plot (x: size data, y: accuracy; labels: every algo to validate).\n",
    "\n",
    "**Concerns when sourcing the label**\n",
    "\n",
    "play with annotators (Data Annotation).\n",
    "\n",
    "problems:\n",
    "\n",
    "1.  Low accuracy annotators.\n",
    "2. Copycat\n",
    "\n",
    "https://newscatcherapi.com/blog/top-6-text-annotation-tools\n",
    "\n",
    "Multi-annotator estimate:\n",
    "\n",
    "- Consensus label = single best label (Simple majority vote - confidence score).\n",
    "- confidence in consensus = how likely is it wrong.\n",
    "- quality of annotator = overall accuracy of their labels.\n",
    "\n",
    "Better methodology: CrowdLab (it should converge with model performance - it could be bat if your model is poorly accurate. take care of it!)\n",
    "\n",
    "https://cleanlab.ai/blog/multiannotator/\n",
    "\n",
    "__Textbook__: Human-in-the-loop Machine Learning (Robert Monarch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 4: Data-centric Evaluation of ML Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function evaluates model predictions for a new example vs its given label.\n",
    "\n",
    "Loss may be function of:\n",
    "\n",
    "1. The predicted class of $\\hat{y} \\in {1, 2,....,k}$ deemed most likely for x.\n",
    "    \n",
    "    Examples of such classification losses: accuracy, balanced accuracy, precision, recall, ...\n",
    "\n",
    "2. The predicted probabilities $p1, p2, ....., pk \\in R^{k}$ of each class for x.\n",
    "\n",
    "    Examples of such classification losses: log loss, AUROC, calibration error..\n",
    "\n",
    "Typical score = average of $Loss(M(x_{i}), y_{i})$ over many examples held-out during training.\n",
    "\n",
    "Alternatives:\n",
    "\n",
    "- Average loss for example from each class separately (e.g per-class accuracy).\n",
    "- Report complete confusion matrix.\n",
    "\n",
    "invest as much as time thinking about this as:\n",
    "- what models  to apply.\n",
    "- how to improve them.\n",
    "\n",
    "Model evaluation has a huge impact in real applications.\n",
    "\n",
    "__Common pitfalls when evaluating models__\n",
    "\n",
    "1. Failing to use truly held-out data (data leakage).\n",
    "2. Reporting only average loss can under-represent severe failure cases for rare examples/subpopulations (misspecified metric).\n",
    "3. Validation data not representative of deployment setting (selection bias).\n",
    "4. Some labels incorrect (annotation error).\n",
    "\n",
    "__Underperforming Subpopulations__\n",
    "\n",
    "define `data slice`: a subset of the dataset that shares common characteristics.\n",
    "\n",
    "`Model predictions should not depend on which slice a datapoint belongs to`\n",
    "\n",
    "Look for improvements:\n",
    "\n",
    "- Over-sample (up-weight) examples from minority subgroup that is receiving poor predictions.\n",
    "- Collect aditional data from the subgroup with poor performance.\n",
    "\n",
    "To see if this has promise (most uncommom):\n",
    "\n",
    "- Re-fit model many versions of dataset with this subgroup down-subsampled to varying  degrees.\n",
    "- Extrapolate the resulting model performance (overall and for subgroups) expected if you had more data from this subgroup.\n",
    "\n",
    "__Discovering underperforming subpopulations__\n",
    "\n",
    "1. Sort examples in the validation data by their loss value, and look at the examples with the high loss for which your model is making the worst prediction (Error Analysis).\n",
    "2. Apply clustering to these examples with high loss to uncover clusters that share common themes amongst these examples.\n",
    "\n",
    "Look for `slice discovery method`, algo that finds slicing functions, which split a dataset into underperforming slices.\n",
    "\n",
    "__why did my model get a particular prediction wrong__\n",
    "\n",
    "1. Given label is incorrect and, our model actually gives the right prediction (recommended action: correct the label).\n",
    "\n",
    "2. Example does not belong to any class or, is fundamentally not predictible (recommended action: toss this example from the dataset, consider adding an \"other\" class if many such examples).\n",
    "\n",
    "3. Example is an outlier (reco: toss example if similar examples would never be seen in deployment) (Can add synthetic data - data augmentation - so model becomes invarient to difference that makes this outlier stand from other examples).\n",
    "\n",
    "There are cases where `model centric` is actually needed, as not all the models can handle all the data patterns in real world. (Otherwise, why look for better algos??)\n",
    "\n",
    "4. Dataset has other examples with nearly identical features but diferent label (reco: define classes more distinctly, meassure extra features to enrich the data).\n",
    "\n",
    "`The key is to practice with the influence functions`: Influence reveals which datapoints have greatest impact on the model. For instance, correcting the label of a mislabeled datapoint with high influence can produce much better model improvement than correcting a mislabeled datapoint that has low influence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 5: Class Imbalance, Outliers, and Distribution Shift"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation metrics__\n",
    "\n",
    "If you’re splitting a dataset into train/test splits, make sure to use stratified data splitting to ensure that the train distribution matches the test distribution (otherwise, you’re creating a distribution shift) problem.\n",
    "\n",
    "With imbalanced data, standard metrics like accuracy might not make sense. There is no one-size-fits-all solution for choosing an evaluation metric: the choice should depend on the problem.\n",
    "\n",
    "__Training models on imbalanced data__\n",
    "\n",
    "Once an evaluation metric has been chosen, you can try training a model in the standard way. If training a model on the true distribution works well, i.e., the model scores highly on the evaluation metric over a held-out test set that matches the real-world distribution, then you’re done!\n",
    "\n",
    "if not, apply:\n",
    "\n",
    "- Sample weights.\n",
    "- Over sampling.\n",
    "- Under sampling.\n",
    "- SMOTE.\n",
    "- Balanced mini batch training: For models trained with mini-batches, like neural networks, when assembling the random subset of data for each mini-batch, you can include datapoints from minority classes with higher probability, such that the mini-batch is balanced. This approach is similar to over-sampling, and it does not throw away data.\n",
    "\n",
    "__Identifying outliers__\n",
    "\n",
    "Some techniques:\n",
    "\n",
    "- Tukey’s fences. A simple method for scalar real-valued data. If Q1 and Q3 are the lower and upper quartiles, then this test says that any observation outside the following range is considered an outlier.\n",
    "\n",
    "- Z-score: The Z-score is the number of standard deviations by which a value is above or below the mean. An outlier is a data point that has a high-magnitude Z-score. You can apply this technique to individual features as well.\n",
    "\n",
    "- isolation forest.\n",
    "- knn distance.\n",
    "- Reconstruction-based methods. Autoencoders are generative models that are trained to compress high-dimensional data into a low-dimensional representation and then reconstruct the original data. If an autoencoder learns a data distribution, then it should be able to encode and then decode an in-distribution data point back into a data point that is close to the original input data. However, for out-of-distribution data, the reconstruction will be worse, so you can use reconstruction loss as a score for detecting outliers.\n",
    "\n",
    "__Distribution shift__\n",
    "\n",
    "Distribution shift is a challenging problem that occurs when the joint distribution of inputs and outputs differs between training and test stages, i.e., This issue is present, to varying degrees, in nearly every practical ML application, in part because it is hard to perfectly reproduce testing conditions at training time.\n",
    "\n",
    "$$\n",
    "P_{train}(X,y) \\not = P_{test}(X,y)\n",
    "$$\n",
    "\n",
    "__types of distribbution shift__\n",
    "\n",
    "**Covariate shift / data shift**\n",
    "\n",
    "Covariate shift occurs when $P(x)$ changes between train and test, but $P(y | x)$ does not. In other words, the distribution of inputs changes between train and test, but the relationship between inputs and outputs does not change.\n",
    "\n",
    "https://dcai.csail.mit.edu/lectures/imbalance-outliers-shift/covariate-shift.svg\n",
    "\n",
    "Examples of covariate shift:\n",
    "\n",
    "- Self-driving car trained on the sunny streets of San Francisco and deployed in the snowy streets of Boston.\n",
    "- Speech recognition model trained on native English speakers and then deployed for all English speakers.\n",
    "- Diabetes prediction model trained on hospital data from Boston and deployed in India.\n",
    "\n",
    "**Concept shift**\n",
    "\n",
    "Concept shift occurs when $P(y | x)$ changes between train and test, but $P(x)$ does not.  In other words, the input distribution does not change, but the relationship between inputs and outputs does. This can be one of the most difficult types of distribution shift to detect and correct.\n",
    "\n",
    "It is tricky to come up with real-world examples of concept shift where there is absolutely no change in $P(x)$:\n",
    "\n",
    "- Making purchase recommendations based on web browsing behavior, trained on pre-pandemic data and deployed in March 2020. Before the pandemic vs during the pandemic, the relationship between browsing behavior and purchases did (e.g., someone who watched lots of travel videos on YouTube before the pandemic might buy plane or hotel tickets, while during the pandemic they might pay for nature documentary movies).\n",
    "\n",
    "**Prior probability shift / label shift**\n",
    "\n",
    "To understand prior probability shift, consider the example of spam classification, where a commonly-used model is Naive Bayes. If the model is trained on a balanced dataset of 50% spam and 50% non-spam emails, and then it’s deployed in a real-world setting where 90% of emails are spam, that is an example of prior probability shift.\n",
    "\n",
    "Another example is when training a classifier to predict diagnoses given symptoms, as the relative prevalence of diseases is changing over time. Prior probability shift shift (rather than covariate shift) is the appropriate assumption to make here, because diseases cause symptoms.\n",
    "\n",
    "**Detecting and addressing distribution shift**\n",
    "\n",
    "Some ways you can detect distribution shift in deployments:\n",
    "\n",
    "- Monitor the performance of your model. Monitor accuracy, precision, statistical measures, or other evaluation metrics. If these change over time, it may be due to distribution shift.\n",
    "- Monitor your data. You can detect data shift by comparing statistical properties of training data and data seen in a deployment.\n",
    "\n",
    "At a high level, distribution shift can be addressed by fixing the data and re-training the model. In some situations, the best solution is to collect a better training set.\n",
    "\n",
    "If unlabeled testing data are available while training, then one way to address covariate shift is to assign individual sample weights to training datapoints to weigh their feature-distribution such that the weighted distribution resembles the feature-distribution of test data. In this setting, even though test labels are unknown, label shift can similarly be addressed by employing shared sample weights for all training examples with the same class label, in order to make the weighted feature-distribution in training data resemble the feature distribution in the test data. However, concept shift cannot be addressed without knowledge of its form in this setting, because there is no way to quantify it from unlabeled test data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 8: Encoding Human Priors: Data Augmentation and Prompt Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Human priors. They are prior knowledge we have about the world, about the data, about the task. And we often take them for granted, like the rotated dog.\n",
    "\n",
    "In the case of the rotated dog, it’s a special type of human prior that is particularly useful. That’s an invariance. Basically a change to the input data that doesn’t change its output. This is useful because we can find smart ways to encode them in the input training data without needing to gather more data.\n",
    "\n",
    "- Encoding — what does that mean? It just means finding a function to represent the invariance. So for rotating the image, it’s just a function for rotation.\n",
    "\n",
    "Specifically, we’re looking at adapting the data today. It’s a very effective place to be doing this and much easier than making architectural or loss function adaptations. It’s a common technique that we ML researchers and practitioners all do.\n",
    "\n",
    "__Human Priors to Augment Training Data__\n",
    "\n",
    "what data augmentation can do is it enables you to encode your human priors over invariances that you know about in your data and you’re able to augment your dataset further, such as using flip and rotation on those dog pictures that you saw previously.\n",
    "\n",
    "Now, those are pretty simple. There are far more advanced methods, such as Mobius transformations (Zhou et al., 2020). If you have classes, you can also use an effective method called Mixup, where you can mix your different classes together to be used as interpolated examples in alpha space (Zhang et al., 2017). What does that mean? If you have dog pictures and cat pictures, you can overlay these images together (e.g. by varying the alpha or A parameter in RGBA). For example, you can change the alpha of a cat image to 60% and the dog image to 40%. You’d get a blended cat-dog, and as a human, you’d agree that there is a cat and dog in it. Then, you could actually change your class label to be 60% cat and 40% dog for your model to predict. You can vary this however you want across your data to produce more training examples with precise labels. This is actually a very effective technique and used pretty widely now.\n",
    "\n",
    "Data augmentation can also be taken to the extreme of synthetic data augmentation. This means using the data you already have, you can even train a model to generate more of that kind or class of data. For this, you can train your own model or you can use foundation models, such as DALL-E or Stable Diffusion in the image scenario, to generate more data from them. Just know that you have to think about how this impacts your test set, if the foundation model has been trained on samples in your test set.\n",
    "\n",
    "__Human Priors at Test-Time (LLMs)__\n",
    "\n",
    "One popular method is called prompt engineering. It is used for large language models (LLMs). What this means is that you are changing the input at test time to elicit certain results at output time. For example, you can ask an LLM to write a letter of recommendation for a student. It’ll write a letter of recommendation that’s pretty average. But if you ask it to write a letter of recommendation for a student who gets into MIT, then it does much better because it assumes your letter will get into MIT.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anexos\n",
    "\n",
    "https://dcai.csail.mit.edu/lectures/\n",
    "https://towardsdatascience.com/outlier-detection-with-autoencoders-6c7ac3e2aa90\n",
    "https://pyod.readthedocs.io/en/latest/\n",
    "https://direct.mit.edu/books/edited-volume/3841/Dataset-Shift-in-Machine-Learning\n",
    "https://dcai.csail.mit.edu/lectures/interpretable-features/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
