createdAt: "2020-12-05T02:28:44.040Z"
updatedAt: "2020-12-07T04:10:14.414Z"
type: "MARKDOWN_NOTE"
folder: "d95c9bc9473d8dba8318"
title: "Design Thinking Workflow Review"
content: '''
  ## Design Thinking Workflow Review
  
  __Process Models, Design Thinking__
  
  - Two advantages of design thinking in data science is that it is applied outside of data science and it encourages the inclusion of domain experts and stakeholders.
  - OSEMN is an example of a simple process model and CRISP-DM an example of a more complex process model.
  - All process models encourage feedback loops for iterative improvement.
  
  ### Data collection
  
  - Stakeholder or domain expert opinion, feasibility and impact are three of the most important factors when prioritizing business opportunities.
  - The notion of degree of belief is important when making statements both in science and in business. No statement has 100% degree of belief, it is some percentage of 100% that is a reflection of accumulated evidence
  
  ### Data Ingestion
  
  - Gather all relevant data from the sources of provided data.
  - Implement several checks for quality assurance.
  - Take the initial steps towards automation of the ingestion pipeline.
  
  __Hills__
  
  Hills are statements  organized according to the principle of “who, what, wow”.
  
  - Who are the end users that will benefit.
  - What are the end users doing or trying to accomplish.
  - Wow, what is the new aspect that will be developed to help them achieve their goal.
  
  
  ### Data Analysis
  
  The principal steps in the process of EDA are:
  
  1. Summarize the data - Generally done using dataframes in R or Python.
  2. Tell the Story - Summarize the details of what connects the dataset to the business opportunity.
  3. Deal with missing data - Identify the strategy for dealing with missing data.
  4. Investigate - Using data visualization and hypothesis testing delve into the relationship between the dataset and the business opportunity.
  5. Communicate - Communicate the findings from the above steps.
  
  __EDA and Data Visualization best practices__
  
  The majority of code for any data science project should be contained within text files. This is a software engineering best practice that ensures re-usability, allows for unit testing and works naturally with version control. >In Python the text files can be executable scripts, modules, a full Python package or some combination of these.
  
  Keep a record of plots and visualization code that you create. It is difficult to remember all of the details of how visualizations were created. Extracting the visualization code to a specific place will ensure that similar plots for future projects will be quick to create.
  
  Use you plots as a quality assurance tool. Given what you know about the data it can be useful to make an educated guess before you execute the cell or run the script. This habit is surprisingly useful for quality assurance of both data and code.
  
  Removing either complete rows or columns in a feature matrix that contain missing values is called `complete case analysis`.
  
  Complete case analysis, although commonly used, can lead to undesirable results—the extent to which depends on the category of missingness.
  
  The best case scenario is that the data are MCAR. It should be noted that imputing values under the other two types of missingness can result in an increase in bias.
  
  In statistics the process of replacing missing data with substituted values is known as `imputation`.
  
  It is a common practice to perform multiple imputations.
  
  One way to deal with that additional uncertainty is to try a range of different values for imputation and measure how the results vary between each set of imputations. This technique is known as `multiple imputation`.
  
  __Hypothesis Testing__
  
  When carrying out a hypothesis test, the central question, null hypothesis and alternative hypothesis should be stated before the data are collected.
  
  Simulation based hypothesis testing like permutation tests provide a flexible alternative to more classical approaches.
  
  The bootstrap can be used to quantify the uncertainty around a parameter estimate and the two combined can be used as an investigative tool.
  
  Bayesian methods bring to the table a number of way to think differently about hypothesis testing. They generally require more time to implement, but the quantification of uncertainty can be useful when making important business decisions.
  
  The t-test is a simple way to care out 1 or 2 sample hypothesis tests.
  
  There are a number of variants on the t-test, but the unequal variances t-test is commonly used.
  
  The t-test and ANOVA (more than two groups) test whether group means have differences between each other.
  
  __Multiple comparisons__
  
  The Bonferroni correction is commonly used to mitigate the multiple comparisons problem, but it is generally too conservative for large data sets.
  
  A number of other methods are available including the Benjamini/Hochberg correction that is based on the false discovery rate.
  
  Permutation experiments are offer an additional method to correct for multiple comparisons that require fewer assumptions.
  
  ### Data transforms and feature engineering
  
  __Class imbalance, data bias__
  
  Imbalanced classes are common especially in specific application scenarios like fraud detection and and customer retention. The first guideline is to ensure that you do not use accuracy as the metric as the results can be misleading. Accuracy is the number of correct calls divided by all of the calls.
  
  The most common approaches to address imbalanced classes are sampling based. Between over-sampling and under-sampling, under-sampling is conceptually simpler. Given a minority class or classes that are noticeably underrepresented, you may randomly drop some of those observations from the training data so that the proportions are more closely matched across classes. A major caveat to under-sampling is that we are not using all of the data.
  
  Over-sampling techniques come in several forms, from random or naive versions to classes of algorithms like the Synthetic Minority Oversampling Technique (SMOTE) [1] and the Adaptive Synthetic (ADASYN) [2] sampling method. There are a number of variants of these over-sampling algorithms that can be compared.
  
  All of the sampling techniques that we discussed are implemented in the imbalanced-learn Python package. This package is convenient because it allows for the implementation of multiple sampling techniques as pipelines. Additionally, it interfaces with TensorFlow and Keras in a convenient way as well, which is important because neural networks are generally sensitive to class imbalance. Support Vector Machines (SVM) are an example of a machine learning algorithm that is less sensitive to imbalanced classes. SVMs can be tuned to accommodate situations with unbalanced class proportions making them a reasonable tool for outlier detection as well.
  
  __Dimensionality reduction__
  
  Applications of data science that often require dimensionality reduction for visualization or for modeling purposes are: image analysis, text analysis, signal processing, astronomy, and medicine. We discuss three main categories of dimensionality reduction techniques: matrix decomposition, manifold learning, and topic models. The techniques developed for topic models generally fall under one of the first two categories, but the application is natural language processing.
  
  We present NMF and Latent Dirichlet Allocation (LDA) as example methods to carry out topic modeling. The embedding approach tSNE is often used to visualize the results of topic model representations in lower dimensional space to both tune the model as well as gather insight into the data. The package pyLDAvis is specifically purposed with visualizing the results of these models.
  
  __CASE STUDY: topic modeling__
  
  Topic modeling is a commonly used form of dimensionality reduction. When we use visualization tools to explore the results of topic modeling we do so to identify features that are relevant to the domain. These insights can be transformed into new features that may be used directly or appended to the feature matrix. We use topic modeling in this case study to enable domain specific feature engineering.
  
  __TUTORIAL: IBM ai360__
  
  The AI Fairness 360 toolkit is an extensible open-source library containing techniques developed by the research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle. The AI Fairness 360 Python package includes metrics to test for biases and algorithms to mitigate bias.
  
  A bias detection and/or mitigation tool needs to be tailored to the particular bias of interest. More specifically, it needs to know the attribute or attributes, called protected attributes, that are of interest: race is one example of a protected attribute, another is age.
  
  __Handling outliers__
  
  **Isolation forest**: One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. This method isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.
  
  **Elliptic envelope**: One common way of performing outlier detection is to assume that data are Gaussian distributed. From this assumption, we can try to define the general shape of the data, which leads to a threshold-approach to calling an observation an outlier.
  
  **OneClassSVM**: Use a single class version of a support vector machine to identify an optimal hyperplane that can be used to call an observation an outlier.
  
  __Model selection Clustering__
  
  **Silhouette Coefficient**: very flexible and can be visualized through a silhouette plot CONS: Generally higher for convex clusters than other clusters such as density based clusters.
  
  **Davies-Bouldin Index**: The computation of Davies-Bouldin is simpler than that of Silhouette scores. CONS: Generally higher for convex clusters than other clusters, such as density based cluster. It is also limited to the distance metrics in Euclidean space.
  
  **Calinski-Harabasz Index**: The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster CONS: Generally higher for convex clusters than other clusters such as density based clusters.
  
  
'''
tags: []
isStarred: false
isTrashed: false
